{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 表情辨識實作流程（Ubuntu 開發環境）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝必要套件\n",
    "# !pip install mediapipe opencv-python matplotlib gradio tensorflow keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mediapipe Face Detection + Bounding Box Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_face(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    mp_face = mp.solutions.face_detection\n",
    "    face_detection = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "    results = face_detection.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    bbox = None\n",
    "    if results.detections:\n",
    "        for det in results.detections:\n",
    "            bboxC = det.location_data.relative_bounding_box\n",
    "            h, w, _ = img.shape\n",
    "            bbox = int(bboxC.xmin * w), int(bboxC.ymin * h), \\\n",
    "                   int(bboxC.width * w), int(bboxC.height * h)\n",
    "            face_crop = img[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
    "            return img, face_crop, bbox\n",
    "    return img, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入 FER-2013 表情分類模型（Keras 範例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 模型讀取與預測\n",
    "def predict_emotion(face_crop, model_path='./model/fer_model_best.h5'):\n",
    "    model = load_model(model_path)\n",
    "    input_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "    input_face = cv2.resize(input_face, (48, 48)) / 255.0\n",
    "    input_face = input_face.reshape(1, 48, 48, 1)\n",
    "    prediction = model.predict(input_face)\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果視覺化（OpenCV / matplotlib）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(img, bbox, prediction):\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    pred_label = emotion_labels[np.argmax(prediction)]\n",
    "    \n",
    "    if bbox:\n",
    "        cv2.rectangle(img, (bbox[0], bbox[1]),\n",
    "                      (bbox[0]+bbox[2], bbox[1]+bbox[3]), (0, 255, 0), 2)\n",
    "        cv2.putText(img, pred_label, (bbox[0], bbox[1] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus：Gradio 圖形介面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744519041.590985  192318 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1744519041.608648  192340 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: D3D12 (NVIDIA GeForce RTX 3070)\n",
      "W0000 00:00:1744519041.620830  192333 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/layers/convolutional/base_separable_conv.py:104: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 248, in from_config\n",
      "    return cls(**config)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/layers/convolutional/separable_conv2d.py\", line 122, in __init__\n",
      "    super().__init__(\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/layers/convolutional/base_separable_conv.py\", line 104, in __init__\n",
      "    super().__init__(\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 288, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to SeparableConv2D: {'groups': 1, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'kernel_constraint': None}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/gradio/blocks.py\", line 1663, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_187455/2355140672.py\", line 8, in gradio_interface\n",
      "    pred = predict_emotion(face_crop)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_187455/4258587962.py\", line 5, in predict_emotion\n",
      "    model = load_model(model_path)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/saving/saving_api.py\", line 196, in load_model\n",
      "    return legacy_h5_format.load_model_from_hdf5(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/legacy/saving/legacy_h5_format.py\", line 133, in load_model_from_hdf5\n",
      "    model = saving_utils.model_from_config(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/legacy/saving/saving_utils.py\", line 88, in model_from_config\n",
      "    return serialization.deserialize_keras_object(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/legacy/saving/serialization.py\", line 495, in deserialize_keras_object\n",
      "    deserialized_obj = cls.from_config(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/models/sequential.py\", line 358, in from_config\n",
      "    layer = saving_utils.model_from_config(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/legacy/saving/saving_utils.py\", line 88, in model_from_config\n",
      "    return serialization.deserialize_keras_object(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/legacy/saving/serialization.py\", line 504, in deserialize_keras_object\n",
      "    deserialized_obj = cls.from_config(cls_config)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/patti/Desktop/Conestoga Courses/ConestogaCollegeWorkplace/venv_wsl/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 250, in from_config\n",
      "    raise TypeError(\n",
      "TypeError: Error when deserializing class 'SeparableConv2D' using config={'name': 'separable_conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 48, 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'pointwise_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'depthwise_regularizer': None, 'pointwise_regularizer': None, 'depthwise_constraint': None, 'pointwise_constraint': None, 'input_shape': [None, 48, 48, 1]}.\n",
      "\n",
      "Exception encountered: Unrecognized keyword arguments passed to SeparableConv2D: {'groups': 1, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'kernel_constraint': None}\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def gradio_interface(image):\n",
    "    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(\"temp.jpg\", image_bgr)\n",
    "    img, face_crop, bbox = detect_face(\"temp.jpg\")\n",
    "    if face_crop is not None:\n",
    "        pred = predict_emotion(face_crop)\n",
    "        visualize_prediction(img, bbox, pred)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "gr.Interface(fn=gradio_interface,\n",
    "             inputs=gr.Image(type=\"numpy\"),\n",
    "             outputs=gr.Image(type=\"numpy\"),\n",
    "             title=\"Facial Expression Classifier\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
