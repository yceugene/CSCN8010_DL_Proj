{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition Workflow (Ubuntu Development Environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mediapipe opencv-python matplotlib gradio tensorflow keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mediapipe Face Detection + Bounding Box Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 23:34:38.163018: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-16 23:34:41.366917: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744860882.402566   80411 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744860882.655409   80411 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744860884.939560   80411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744860884.939590   80411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744860884.939591   80411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744860884.939592   80411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-16 23:34:45.162672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_face(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    mp_face = mp.solutions.face_detection\n",
    "    face_detection = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "    results = face_detection.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    bbox = None\n",
    "    if results.detections:\n",
    "        for det in results.detections:\n",
    "            bboxC = det.location_data.relative_bounding_box\n",
    "            h, w, _ = img.shape\n",
    "            bbox = int(bboxC.xmin * w), int(bboxC.ymin * h), \\\n",
    "                   int(bboxC.width * w), int(bboxC.height * h)\n",
    "            face_crop = img[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
    "            return img, face_crop, bbox\n",
    "    return img, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FER-2013 Emotion Classification Model (Keras Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load model and make predictions\n",
    "def predict_emotion(face_crop, model_path='./model/fer_model_best.h5'):\n",
    "    model = load_model(model_path)\n",
    "    input_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "    input_face = cv2.resize(input_face, (48, 48)) / 255.0\n",
    "    input_face = input_face.reshape(1, 48, 48, 1)\n",
    "    prediction = model.predict(input_face)\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize（OpenCV / matplotlib）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(img, bbox, prediction):\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    pred_label = emotion_labels[np.argmax(prediction)]\n",
    "    \n",
    "    if bbox:\n",
    "        cv2.rectangle(img, (bbox[0], bbox[1]),\n",
    "                      (bbox[0]+bbox[2], bbox[1]+bbox[3]), (0, 255, 0), 2)\n",
    "        cv2.putText(img, pred_label, (bbox[0], bbox[1] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Gradio Graphical Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def gradio_interface(image):\n",
    "    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(\"temp.jpg\", image_bgr)\n",
    "    img, face_crop, bbox = detect_face(\"temp.jpg\")\n",
    "    if face_crop is not None:\n",
    "        pred = predict_emotion(face_crop)\n",
    "        visualize_prediction(img, bbox, pred)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "gr.Interface(fn=gradio_interface,\n",
    "             inputs=gr.Image(type=\"numpy\"),\n",
    "             outputs=gr.Image(type=\"numpy\"),\n",
    "             title=\"Facial Expression Classifier\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
